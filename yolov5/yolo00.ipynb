{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2023-7-22 Python-3.10.11 torch-2.0.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24576MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 270 layers, 7235389 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "c:\\ProgramData\\Anaconda3\\envs\\Python3_10\\lib\\site-packages\\yolov5\\models\\common.py:522: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\Python3_10\\lib\\site-packages\\yolov5\\models\\yolo.py:64: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import yolov5\n",
    "import torch.onnx as torch_onnx\n",
    "\n",
    "# load pretrained model\n",
    "model = yolov5.load('yolov5s.pt')\n",
    "# set model parameters\n",
    "model.conf = 0.25  # NMS confidence threshold\n",
    "model.iou = 0.45  # NMS IoU threshold\n",
    "model.agnostic = False  # NMS class-agnostic\n",
    "model.multi_label = False  # NMS multiple labels per box\n",
    "model.max_det = 50  # maximum number of detections per image\n",
    "\n",
    "model.to('cpu')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create a dummy input tensor of the same size as your input images\n",
    "x = torch.randn(1, 3, 640, 640, requires_grad=True)\n",
    "\n",
    "# Specify the name of the output ONNX file\n",
    "onnx_file_name = \"yolov5s.onnx\"\n",
    "\n",
    "# Export the model to an ONNX file\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  x,                   # model input (or a tuple for multiple inputs)\n",
    "                  onnx_file_name,      # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,  # store the trained parameter weights inside the model file\n",
    "                  opset_version=11,    # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ONNX model\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as rt\n",
    "import torchvision\n",
    "\n",
    "coco_labels = [\n",
    "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant',\n",
    "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n",
    "    'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "    'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\n",
    "    'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven',\n",
    "    'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "    'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "# Load the ONNX model\n",
    "sess = rt.InferenceSession(\"yolov5s.onnx\")\n",
    "\n",
    "# Open the video stream\n",
    "cap = cv2.VideoCapture(0)  # 0 for default camera, or 'path_to_video_file' for a video file\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the image for YOLOv5\n",
    "    # Resize to the input size expected by the model, convert to RGB, normalize, add batch dimension\n",
    "    input_img = cv2.cvtColor(cv2.resize(frame, (640, 640)), cv2.COLOR_BGR2RGB)\n",
    "    input_img = input_img / 255.0\n",
    "    input_img = np.transpose(input_img, (2, 0, 1))\n",
    "    input_img = np.expand_dims(input_img, 0).astype(np.float32)\n",
    "\n",
    "    # Perform inference\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    outputs = sess.run(None, {input_name: input_img})\n",
    "\n",
    "    # Here outputs are the raw YOLOv5 outputs. You would need to post-process them\n",
    "    # to get bounding box coordinates, class labels, etc.\n",
    "\n",
    "    # Parse outputs\n",
    "    for output in outputs:\n",
    "        boxes = output[0, :, :4]\n",
    "        objectness = output[0, :, 4]\n",
    "        class_probs = output[0, :, 5:]\n",
    "        \n",
    "        # Select top 10 objects based on objectness score\n",
    "        top_indices = np.argsort(objectness)[-4:]\n",
    "        top_boxes = boxes[top_indices]\n",
    "        top_class_probs = class_probs[top_indices]\n",
    "        top_classes = np.argmax(top_class_probs, axis=1)\n",
    "        # print(top_boxes)\n",
    "\n",
    "        # Draw bounding boxes and class labels on the frame\n",
    "        for i in range(4):\n",
    "            box = top_boxes[i]\n",
    "            class_id = top_classes[i]\n",
    "\n",
    "            # The box coordinates need to be rescaled to the original frame size: 640*640 input tensor -> 640*480 frame\n",
    "            x1, y1, x2, y2 = int(box[0] - box[2]/2), int(box[1] - box[3]/2), int(box[0] + box[2]/2), int(box[1] + box[3]/2)\n",
    "\n",
    "            y1 = int(y1 * (480/640))\n",
    "            y2 = int(y2 * (480/640))\n",
    "\n",
    "            # Draw the bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            # print(x1, y1, x2, y2)\n",
    "\n",
    "            # Draw the class label\n",
    "            label = coco_labels[class_id]\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
