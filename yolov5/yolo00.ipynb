{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import yolov5\n",
    "import onnx\n",
    "import torch.onnx as torch_onnx\n",
    "\n",
    "# load pretrained model\n",
    "model = yolov5.load(\"yolov5s.pt\")\n",
    "# set model parameters\n",
    "model.conf = 0.25  # NMS confidence threshold\n",
    "model.iou = 0.45  # NMS IoU threshold\n",
    "model.agnostic = False  # NMS class-agnostic\n",
    "model.multi_label = False  # NMS multiple labels per box\n",
    "model.max_det = 50  # maximum number of detections per image\n",
    "\n",
    "model.to(\"cpu\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create a dummy input tensor of the same size as your input images\n",
    "x = torch.randn(1, 3, 640, 640, requires_grad=True)\n",
    "\n",
    "# Specify the name of the output ONNX file\n",
    "onnx_file_name = \"yolov5s.onnx\"\n",
    "\n",
    "# Export the model to an ONNX file\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  x,                   # model input (or a tuple for multiple inputs)\n",
    "                  onnx_file_name,      # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,  # store the trained parameter weights inside the model file\n",
    "                  opset_version=11,    # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = [\"input\"],   # the model\"s input names\n",
    "                  output_names = [\"output\"], # the model\"s output names\n",
    "                  dynamic_axes={\"input\" : {0 : \"batch_size\"},    # variable length axes\n",
    "                                \"output\" : {0 : \"batch_size\"}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantize\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import QuantType\n",
    "from onnxruntime.quantization.quantize import quantize_dynamic\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_file_name = 'yolov5s.onnx'\n",
    "onnx_model_quantized_path = 'yolov5s_qt.onnx'\n",
    "onnx_model = onnx.load(onnx_file_name)\n",
    "\n",
    "# Specify the quantization method and parameters\n",
    "quantize_dynamic(\n",
    "    model_input=onnx_file_name,\n",
    "    model_output=onnx_model_quantized_path,\n",
    "    optimize_model=True,\n",
    "    per_channel=False,\n",
    "    reduce_range=False,\n",
    "    weight_type=QuantType.QUInt8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ONNX model\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as rt\n",
    "import torchvision\n",
    "\n",
    "coco_labels = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\",\n",
    "    \"train\", \"truck\", \"boat\", \"traffic light\", \"fire hydrant\",\n",
    "    \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\",\n",
    "    \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\",\n",
    "    \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\",\n",
    "    \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\",\n",
    "    \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\",\n",
    "    \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\",\n",
    "    \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n",
    "    \"chair\", \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\",\n",
    "    \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\",\n",
    "    \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "    \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "]\n",
    "\n",
    "# Load the ONNX model\n",
    "sess = rt.InferenceSession(\"yolov5s_qt.onnx\")\n",
    "\n",
    "# Open the video stream\n",
    "cap = cv2.VideoCapture(0)  # 0 for default camera, or \"path_to_video_file\" for a video file\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # debug: open an image instead\n",
    "    #frame = cv2.imread(\"F:\\\\Hacarus\\\\SAM_zhu\\\\notebooks\\\\images\\\\bike0.jpg\")\n",
    "\n",
    "    # Preprocess the image for YOLOv5\n",
    "    # Resize to the input size expected by the model, convert to RGB, normalize, add batch dimension\n",
    "    input_img = cv2.cvtColor(cv2.resize(frame, (640, 640)), cv2.COLOR_BGR2RGB)\n",
    "    input_img = input_img / 255.0\n",
    "    input_img = np.transpose(input_img, (2, 0, 1))\n",
    "    input_img = np.expand_dims(input_img, 0).astype(np.float32)\n",
    "\n",
    "    # Perform inference\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    outputs = sess.run(None, {input_name: input_img})\n",
    "\n",
    "    # Here outputs are the raw YOLOv5 outputs. You would need to post-process them\n",
    "    # to get bounding box coordinates, class labels, etc.\n",
    "\n",
    "    # Parse outputs\n",
    "    for output in outputs:\n",
    "        boxes = output[0, :, :4]\n",
    "        objectness = output[0, :, 4]\n",
    "        class_probs = output[0, :, 5:]\n",
    "        \n",
    "        # Select top 4 objects based on objectness score\n",
    "        top_indices = np.argsort(objectness)[-1:]\n",
    "        top_boxes = boxes[top_indices]\n",
    "        top_class_probs = class_probs[top_indices]\n",
    "        top_classes = np.argmax(top_class_probs, axis=1)\n",
    "\n",
    "        # Draw bounding boxes and class labels on the frame\n",
    "        for i in range(1):\n",
    "            box = top_boxes[i]\n",
    "            class_id = top_classes[i]\n",
    "\n",
    "            # The box coordinates need to be rescaled to the original frame size: 640*640 input tensor -> 640*480 frame\n",
    "            x1, y1, x2, y2 = int(box[0] - box[2]/2), int(box[1] - box[3]/2), int(box[0] + box[2]/2), int(box[1] + box[3]/2)\n",
    "\n",
    "            y1 = int(y1 * (480/640))\n",
    "            y2 = int(y2 * (480/640))\n",
    "\n",
    "            # Draw the bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # Draw the class label\n",
    "            label = coco_labels[class_id]\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    \n",
    "    # Break the loop on \"q\" key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
